{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "patient-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "independent-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "above-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "tokenizer = torchtext.data.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brief-continuity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_text = \"Hello this is harish\"\n",
    "temp_text = tokenizer(temp_text)\n",
    "temp_text = vectorizer.get_vecs_by_tokens(temp_text, lower_case_backup=True)\n",
    "temp_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focused-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarityDataset(Dataset):\n",
    "    def __init__(self, tokenizer, vectorizer):\n",
    "        self.data = pd.read_csv('questions.csv')\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.y = torch.from_numpy(self.data['is_duplicate'].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tx1 = self.tokenizer(str(self.data['question1'].values[index]))\n",
    "        tx2 = self.tokenizer(str(self.data['question2'].values[index]))\n",
    "        x1 = vectorizer.get_vecs_by_tokens(tx1, lower_case_backup=True)\n",
    "        x2 = vectorizer.get_vecs_by_tokens(tx2, lower_case_backup=True)\n",
    "        return x1, x2, self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "guided-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextSimilarityDataset(tokenizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reported-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(batch):\n",
    "    score = []\n",
    "    q1 = []\n",
    "    q2 = []\n",
    "    max_pad_q1 = 0\n",
    "    max_pad_q2 = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        if max_pad_q1 < batch[i][0].shape[0]:\n",
    "            max_pad_q1 = batch[i][0].shape[0]\n",
    "\n",
    "        if max_pad_q2 < batch[i][1].shape[0]:\n",
    "            max_pad_q2 = batch[i][1].shape[0]\n",
    "\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        \n",
    "        if batch[i][0].shape[0] == max_pad_q1:\n",
    "            q1.append(batch[i][0])\n",
    "        else:\n",
    "            q1_temp = torch.zeros(max_pad_q1, 50)\n",
    "            q1_temp[:batch[i][0].shape[0], :] = batch[i][0]\n",
    "            q1.append(q1_temp)\n",
    "        \n",
    "        if batch[i][1].shape[0] == max_pad_q2:\n",
    "            q2.append(batch[i][1])\n",
    "        else:\n",
    "            q2_temp = torch.zeros(max_pad_q2, 50)\n",
    "            q2_temp[:batch[i][1].shape[0], :] = batch[i][1]\n",
    "            q2.append(q2_temp)\n",
    "        \n",
    "        score.append(batch[i][2])\n",
    "        \n",
    "    return torch.stack(q1), torch.stack(q2), torch.stack(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crude-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(dataset=dataset, batch_size=32, shuffle=True, collate_fn=gen_dataset, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regulation-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        out, _ = self.rnn(x)\n",
    "        return out\n",
    "        \n",
    "    def calc_dist(self, x1, x2):\n",
    "        x1 = x1[-1:,-1:]\n",
    "        x2 = x2[-1:,-1:]\n",
    "        return torch.exp(-(torch.norm(x1 - x2))).to(device)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_one(x1)\n",
    "        x2 = self.forward_one(x2)\n",
    "        out = torch.zeros(x1.shape[0]).to(device)\n",
    "        for i in range(x1.shape[0]):\n",
    "            out[i] = self.sigmoid(self.calc_dist(x1[i], x2[i]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "standard-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper parameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "hidden = 1024\n",
    "layers = 2\n",
    "dropout = 0.5\n",
    "inpt_size = 50\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alleged-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseLSTM(inpt_size, embed_size, hidden, layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.tensorboard.Summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "public-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continuous-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"siamese-new.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "broadband-supervisor",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-bb65f64c68ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-100-e0fb237ef575>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-100-e0fb237ef575>\u001b[0m in \u001b[0;36mforward_one\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harish\\desktop\\ml-api\\pip_modules\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harish\\desktop\\ml-api\\pip_modules\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    145\u001b[0m         return F.embedding(\n\u001b[0;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harish\\desktop\\ml-api\\pip_modules\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    count = 0\n",
    "    for _, batch in enumerate(train_iterator):\n",
    "        q1, q2, score = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        output = model.forward(q1, q2)\n",
    "        score = score.type(torch.cuda.FloatTensor)\n",
    "        loss = err(output, score)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print(f'{epoch+1} epoch of {num_epochs} epochs - {count+1} batch - Current loss is {loss}')\n",
    "    torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "interested-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x1, x2):\n",
    "    x1 = tokenizer(x1)\n",
    "    x1 = vectorizer.get_vecs_by_tokens(x1, lower_case_backup=True)\n",
    "    x2 = tokenizer(x2)\n",
    "    x2 = vectorizer.get_vecs_by_tokens(x2, lower_case_backup=True)\n",
    "    x1 = torch.stack([x1]).to(device)\n",
    "    x2 = torch.stack([x2]).to(device)\n",
    "    out = model.forward(x1, x2)\n",
    "    print(out[0])\n",
    "    if out[0] > 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "overhead-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7311, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "out = predict(model, \"Lion looks for its den\", \"Kite flies over the sky\")\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
