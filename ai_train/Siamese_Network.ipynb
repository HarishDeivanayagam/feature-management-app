{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "0vQ7BJWbscL8",
    "outputId": "1d512307-6feb-4ab7-fa75-10739ceb4ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import torch\n",
    "import gensim\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print ('Device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ods4Q-Mu6I_"
   },
   "source": [
    "**Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFa9NaC3uXYX"
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = 'questions.csv'\n",
    "EMBEDDING_PATH = 'GoogleNews-vectors-negative300.bin'\n",
    "EMBEDDING_DIMENSION = 300\n",
    "EMBEDDING_REQUIRES_GRAD = False\n",
    "HIDDEN_CELLS = 50\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZR1adlgvJ-H"
   },
   "source": [
    "**Load Train File and check the distribution of Duplicate Questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "d-Tq65kMu3MT",
    "outputId": "f2eb5f27-e95b-42a3-8fe8-ab1aecf3baf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Duplicate Questions Pair:  36.92484994472624\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAIN_FILE_PATH)\n",
    "print ('Percentage of Duplicate Questions Pair: ', df_train['is_duplicate'].mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbxRoVYfPvkH"
   },
   "source": [
    "**Data Cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQplM1Wuvb0y"
   },
   "outputs": [],
   "source": [
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjVhpVA-P0R6"
   },
   "source": [
    "**Convert train data into list of tuples where each tuple is of the form (question1, question2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "InlnbLqtJMNn",
    "outputId": "d24ec85f-b47d-4fb0-8315-cb0c5fc798e0"
   },
   "outputs": [],
   "source": [
    "train_questions_pair = []\n",
    "train_labels = []\n",
    "for _, row in df_train.iterrows():\n",
    "    \n",
    "    q1 = text_to_wordlist(str(row['question1']))\n",
    "    q2 = text_to_wordlist(str(row['question2']))\n",
    "    label = int(row['is_duplicate'])\n",
    "    if q1 and q2:\n",
    "        train_questions_pair.append((\n",
    "                q1, q2\n",
    "            ))\n",
    "        train_labels.append(label)\n",
    "\n",
    "print ('Train Data Question Pairs: ', len(train_questions_pair))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJQ41TV1P-pe"
   },
   "source": [
    "**Create a Language class that will keep track of the dataset vocabulary and corresponding indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCVIxW8yJ86O"
   },
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words + 1\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words + 1] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "language = Language()\n",
    "for data in [train_questions_pair]:\n",
    "    for question_pair in data:\n",
    "        q1 = question_pair[0]\n",
    "        q2 = question_pair[1]\n",
    "        language.addSentence(q1)\n",
    "        language.addSentence(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23Lh5-d4QOdb"
   },
   "source": [
    "**Create a dataset class which can be indexed to retrieve Questions Pair along with corresponding Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQigzxPUKEPA"
   },
   "outputs": [],
   "source": [
    "class QuestionsDataset(Dataset):\n",
    "    def __init__(self, questions_list, word2index, labels):\n",
    "        self.questions_list = questions_list\n",
    "        self.labels = labels\n",
    "        self.word2index = word2index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        questions_pair = self.questions_list[index]\n",
    "        q1 = questions_pair[0]\n",
    "        q1_indices = []\n",
    "        for word in q1.split():\n",
    "            q1_indices.append(self.word2index[word])\n",
    "            \n",
    "        q2 = question_pair[1]\n",
    "        q2_indices = []\n",
    "        for word in q2.split():\n",
    "            q2_indices.append(self.word2index[word])\n",
    "            \n",
    "        # q1_indices and q2_indices are lists of indices against words used in the sentence \n",
    "        return q1_indices, q2_indices, self.labels[index]\n",
    "    \n",
    "train_dataset = QuestionsDataset(train_questions_pair, language.word2index, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "OW4ChjLUKIwo",
    "outputId": "d45300d7-8756-4981-9ed3-9da4e3e7dc6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Vocabulary Words:  86020\n"
     ]
    }
   ],
   "source": [
    "n_vocabulary_words = len(language.word2index)\n",
    "print ('Total Unique Vocabulary Words: ', n_vocabulary_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEBZZgTZSk1C"
   },
   "source": [
    "**Custom Collate is implemented to adjust the data in the desired format and calculate lengths which will later be used for padding and packing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vl1mkUSSKJbe"
   },
   "outputs": [],
   "source": [
    "class CustomCollate:\n",
    "    def custom_collate(self, batch):\n",
    "\n",
    "        # batch = list of tuples where each tuple is of the form ([i1, i2, i3], [j1, j2, j3], label)\n",
    "        q1_list = []\n",
    "        q2_list = []\n",
    "        labels = []\n",
    "        for training_example in batch:\n",
    "          q1_list.append(training_example[0])\n",
    "          q2_list.append(training_example[1])\n",
    "          labels.append(training_example[2])\n",
    "          \n",
    "        \n",
    "        q1_lengths = [len(q) for q in q1_list]\n",
    "        q2_lengths = [len(q) for q in q2_list]\n",
    "        \n",
    "        return q1_list, q1_lengths, q2_list, q2_lengths, labels\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.custom_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5Rz6D_NSyK-"
   },
   "source": [
    "**Split Training Data into Train and Validation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "iS1mEeNFKOKS",
    "outputId": "25383e62-e894-4cd2-84cd-d4662170b540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size 323465, Validation Set Size 80866\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.2\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "shuffle_dataset = True\n",
    "random_seed = 32\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, sampler=train_sampler, collate_fn=CustomCollate())\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, sampler=validation_sampler, collate_fn=CustomCollate())\n",
    "\n",
    "print ('Training Set Size {}, Validation Set Size {}'.format(len(train_indices), len(val_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4asiGcQPS5-Y"
   },
   "source": [
    "**Create Embeding Matrix for the dataset vocabulary using pre-trained Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ96ZAgSKQjl"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained embeddings from word2vec\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_PATH, binary=True)\n",
    "# Convert word2vec embeddings into FloatTensor\n",
    "word2vec_weights = torch.FloatTensor(word2vec_model.vectors)\n",
    "\n",
    "# Create a random weight tensor of the shape (n_vocabulary_words + 1, EMBEDDING_DIMENSION) and place each word's embedding from word2vec at the index assigned to that word\n",
    "# Two key points:\n",
    "# 1. Weights tensor has been initialized randomly so that the words which are part of our dataset vocabulary but are not present in word2vec are given a random embedding\n",
    "# 2. Embedding at 0 index is all zeros. This is the embedding for the padding that we will do for batch processing\n",
    "weights = torch.randn(n_vocabulary_words + 1, EMBEDDING_DIMENSION)\n",
    "weights[0] = torch.zeros(EMBEDDING_DIMENSION)\n",
    "for word, lang_word_index in language.word2index.items():\n",
    "    if word in word2vec_model:\n",
    "        weights[lang_word_index] = torch.FloatTensor(word2vec_model.word_vec(word))\n",
    "\n",
    "del word2vec_model\n",
    "del word2vec_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXfiuDHcUOvN"
   },
   "source": [
    "**Siamese Network with single LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrPIO95oKW06"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, pretrained_weights):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Creating embedding object from the pre-trained weights\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_weights)\n",
    "        self.embedding.weight.requires_grad = EMBEDDING_REQUIRES_GRAD\n",
    "        # Create a single LSTM since this is a Siamese Network and the weights are shared\n",
    "        self.lstm = nn.LSTM(input_size=EMBEDDING_DIMENSION, hidden_size=HIDDEN_CELLS, num_layers = NUM_LAYERS, batch_first = True)\n",
    "    \n",
    "    # Manhattan Distance Calculator\n",
    "    def exponent_neg_manhattan_distance(self, x1, x2):\n",
    "        return torch.exp(-torch.sum(torch.abs(x1 - x2), dim=0)).to(device)\n",
    "\n",
    "    def forward_once(self, x, input_lengths):\n",
    "      \n",
    "        # x is of the shape (batch_dim, sequence)\n",
    "        # e.g. x = [\n",
    "        #  [i1, i2, i3],\n",
    "        #  [j1, j2, j3, j4]\n",
    "        # ]\n",
    "        \n",
    "        # input_lengths is the list that contains the sequence lengths for each sequence\n",
    "        # e.g. input_lengths = [3, 4]\n",
    "        \n",
    "        # Reverse sequence lengths indices in decreasing order as per the requirement from PyTorch before Padding and Packing\n",
    "        sorted_indices = np.flipud(np.argsort(input_lengths))\n",
    "        input_lengths = np.flipud(np.sort(input_lengths))\n",
    "        input_lengths = input_lengths.copy() # https://github.com/facebookresearch/InferSent/issues/99\n",
    "        \n",
    "        # Reorder questions in the decreasing order of their lengths\n",
    "        ordered_questions = [torch.LongTensor(x[i]).to(device) for i in sorted_indices]\n",
    "        # Pad sequences with 0s to the max length sequence in the batch\n",
    "        ordered_questions = torch.nn.utils.rnn.pad_sequence(ordered_questions, batch_first=True)\n",
    "        # Retrieve Embeddings\n",
    "        embeddings = self.embedding(ordered_questions).to(device)\n",
    "        # Pack the padded sequences and pass it through LSTM\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeddings, input_lengths, batch_first=True)\n",
    "        out, (hn, cn) = self.lstm(packed)\n",
    "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=int(input_lengths[0]))\n",
    "        \n",
    "        # The following step reorders the calculated activations to the original order in which questions were passed\n",
    "        result = torch.FloatTensor(unpacked.size())\n",
    "        for i, encoded_matrix in enumerate(unpacked):\n",
    "            result[sorted_indices[i]] = encoded_matrix\n",
    "        return result\n",
    "\n",
    "    def forward(self, q1, q1_lengths, q2, q2_lengths):\n",
    "        output1 = self.forward_once(q1, q1_lengths)\n",
    "        output2 = self.forward_once(q2, q2_lengths)\n",
    "        similarity_score = torch.zeros(output1.size()[0]).to(device)\n",
    "        # Calculate Similarity Score between both questions in a single pair\n",
    "        for index in range(output1.size()[0]):\n",
    "            # Sequence lenghts are being used to index and retrieve the activations before the zero padding since they were not part of original question\n",
    "            q1 = output1[index, q1_lengths[index] - 1, :]\n",
    "            q2 = output2[index, q2_lengths[index] - 1, :]\n",
    "            similarity_score[index] = self.exponent_neg_manhattan_distance(q1, q2)\n",
    "        return similarity_score\n",
    "    \n",
    "model = SiameseNetwork(weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVGXZEeIWG2C"
   },
   "source": [
    "**Create a Loss function and an Optimizer. In this case, we have created Mean Squared Error as our Loss function and Adam as an Optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttT0CLSBKZYt"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01 )\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"down-siamese.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qn8rNRAlXFV8"
   },
   "source": [
    "**Let's train the model !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNjVT6M2KcM3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[420, 2, 577, 7, 867, 3, 1327, 1988], [1, 119, 3, 65272, 112, 11367, 4928], [44, 80, 32, 2520, 6425, 143, 26568], [59, 2, 3, 199, 1616, 7, 3340, 1718], [28, 7, 134, 37, 13069, 7060], [44, 32, 1936, 10123, 29, 982, 2404, 7, 1248, 112, 326, 3152], [18, 45, 656, 327, 320, 67, 288, 216, 3237, 32, 1113, 1766, 18, 288, 907, 422, 327, 7725, 1, 88, 18, 101], [1, 2, 3, 199, 254, 7, 5959, 32, 2625], [28, 101, 18, 2713, 4390, 67, 623], [28, 101, 103, 28251, 87, 17542, 5177, 74, 3738, 112, 3958, 6295, 31697], [1, 2, 3, 1188, 14, 8098], [18, 45, 3188, 37, 695, 1149, 5668, 352, 222, 101, 18, 348, 1149, 21150, 1620], [1, 119, 120, 201, 763, 112, 1094, 1342], [80, 1139, 8995, 5317, 42, 25038, 11489], [1, 2, 3, 55368, 14, 3, 2182, 196, 9, 3411, 3412], [28, 101, 103, 1462, 37, 12470, 10586, 1492, 7, 3340, 37, 29071], [53, 2, 3, 199, 228, 7, 1467, 479, 37, 18489], [1, 2, 3, 282, 10012, 266, 83, 215, 37, 643, 1371, 147], [59, 2394, 3584, 2, 267, 17, 58716, 171, 36708], [1, 2, 3, 199, 2531, 2555, 772], [28, 334, 3626, 9890, 27285, 27286, 165, 78], [1, 101, 103, 304, 14, 1074, 2298, 165, 3, 1717, 14, 1253, 67, 3520, 1260, 1255], [18, 405, 166, 23295, 148, 95, 107, 37, 18, 177, 7, 464, 112, 37, 149, 150, 29, 120, 866, 107, 434, 464, 112, 32, 149, 150], [28, 168, 1989, 157, 4044, 37, 1454, 169, 3, 24, 164, 420, 119, 3165, 7, 1212, 294, 1944, 9, 3233], [29, 103, 23330, 37, 695, 1386, 150], [1, 119, 120, 102, 33, 1182, 2953], [1, 119, 120, 99, 491, 7, 51, 592, 23, 212, 2, 1961, 498, 3529, 610, 6624], [334, 157, 750, 82, 420, 119, 103, 577, 7, 2444, 84, 171, 139, 317, 1177], [1, 2, 3, 199, 674, 112, 245, 2776], [2, 50, 2459, 7, 307, 8732, 67, 10650, 349], [1, 2, 3, 1875, 196, 119, 187, 1174, 7, 2217, 1445, 706], [2, 50, 471, 81, 3, 1891, 14, 5601, 479, 1191, 1189, 29, 288, 39, 16696, 23, 3, 4447, 2, 7663]]\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "# Threshold 0.5. Since similarity score will be a value between 0 and 1, we will consider all question pair with values greater than threshold as Duplicate\n",
    "threshold = torch.Tensor([0.5]).to(device)\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    loss_history = []\n",
    "    model.train(True)\n",
    "    train_correct_total = 0\n",
    "    for i, (q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths, labels) in enumerate(train_loader):\n",
    "        print(q1_batch)\n",
    "\n",
    "        labels = torch.FloatTensor(labels).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        similarity_score = model(q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths)\n",
    "        predictions = (similarity_score > threshold).float() * 1\n",
    "        total = labels.size()[0]\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        train_correct_total += correct\n",
    "        \n",
    "        loss = criterion(similarity_score, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            loss_history.append(loss.item())\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, np.mean(loss_history), (correct / total) * 100))\n",
    "            \n",
    "    print('Training Loss: {:.4f}, Training Accuracy: {:.4f}'.format(np.mean(loss_history), (train_correct_total / len(train_indices)) * 100))\n",
    "    \n",
    "    model.train(False)\n",
    "    val_correct_total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths, labels) in enumerate(val_loader):\n",
    "\n",
    "            labels = torch.FloatTensor(labels).to(device)\n",
    "\n",
    "            similarity_score = model(q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths)\n",
    "            predictions = (similarity_score > threshold).float() * 1\n",
    "            total = labels.size()[0]\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            val_correct_total += correct\n",
    "        \n",
    "        avg_acc_val =  val_correct_total * 100 / len(val_indices)\n",
    "        print ('Validation Set Size {}, Correct in Validation {}, Validation Accuracy {:.2f}%'.format(len(val_indices), val_correct_total, avg_acc_val))\n",
    "        \n",
    "        \n",
    "    torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Siamese Network",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
